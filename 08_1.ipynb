{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrsjEtJ3TcWP9vWlBd9J78",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chocoding1/Machine_Learning_Deep_Learning/blob/main/08_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **합성곱 신경망의 구성 요소**"
      ],
      "metadata": {
        "id": "yIxgxIFdKVAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 합성곱\n",
        "\n",
        "합성곱(convolution)은 인공 신경망의 밀집층 연산처럼 입력 데이터 전체에 가중치를 적용하는 것이 아니라 일부에만 가중치를 곱한다.<br>\n",
        "**합성곱 신경망(convolutional neural network, CNN)**에서는 완전 연결 신경망과 달리 뉴런을 **필터** 혹은 **커널**이라고도 부른다. (뉴런 = 필터 = 커널)<br>\n",
        "굳이 따지자면 뉴런이 필터, 필터에서 말하는 가중치가 커널\n",
        "\n",
        "*완전 연결 신경망 : 완전 연결 층(밀집층)만 사용하여 만든 신경망\n",
        "\n",
        "즉 이 필터가 뉴런을 몇 개씩 돌며, 출력을 만든다.<br>\n",
        "합성곱의 1차원이 아니라 2차원 입력에도 적용할 수 있다는 것이 장점이다.<br>\n",
        "<img src = \"https://wikidocs.net/images/page/64066/conv5.png\" height = 200 width = 550><br>\n",
        "입력이 2차원 배열이면 필터(커널)도 2차원이어야 한다. 위 그림에서 이 커널의 크기는 (3, 3)으로 가정한다.(커널의 크기는 하이퍼파라미터)\n",
        "\n",
        "왼쪽 위 모서리에서부터 합성곱을 실시하는데, 입력의 9개 원소와 커널의 9개 가중치를 곱한 후(물론 여기에서도 절편을 더한다) 1개의 출력을 만든다.<br>\n",
        "그 다음에는 필터가 오른쪽으로 한 칸 이동하여 합성곱을 또 수행한다. 그렇게 왼쪽 위에서 오른쪽 맨 아래까지 이동하면서 출력을 만든다.<br>\n",
        "계산식은 밀집층과 크게 다르지 않다. 입력과 가중치의 행과 열을 맞추어 곱셈하고 모두 더하는 게 전부이다.\n",
        "\n",
        "이렇게 되면 (5, 5) 크기의 입력이 (3, 3) 크기로 압축되고, 이 압축된 출력을 **특성 맵(feature map)**이라고 부른다.\n",
        "\n",
        "밀집층에서 여러 개의 뉴런을 사용하듯이 합성곱 층에서도 여러 개의 필터를 사용한다.<br>\n",
        "<img src = \"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Fbb1c5547-2b0c-4d9c-9d6d-d821c4428097%2Fimage.png\" height = 350 width = 600><br>\n",
        "위 그럼처럼 여러 개의 필터를 사용하면 만들어진 feature map은 순서대로 차곡차곡 쌓인다. (2, 2) feature map을 쌓으면 3차원 배열이 된다. 위 그림에서는 (2, 2) 크기의 feature map이 쌓여 (2, 2, 3) 크기의 3차원 배열이 된다.<br>\n",
        "또한 당연하지만 밀집층에 있는 뉴런의 가중치가 모두 다르듯이 합성곱 층에 있는 필터의 가중치(커널)도 모두 다르다.\n",
        "\n",
        "즉 합성곱은 실제 계산은 밀집층과 동일하게 단순히 입력과 가중치를 곱하는 것이지만, 2차원 형태를 유지하는 점이 다른다. 이렇게 2차원 구조를 그대로 사용하기 때문에 합성곱 신경망이 이미지 처리 분야에서 뛰어난 성능을 발휘한다."
      ],
      "metadata": {
        "id": "LvJvXMwTKpFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 케라스 합성곱 층\n",
        "\n",
        "케라스의 층은 모두 keras.layers 패키지 아래에 클래스로 구현되어 있다. 합성곱 층도 마찬가지인데, 특별히 입력 위를 (왼쪽 위에서 오른쪽 아래로) 이동하는 합성곱은 Conv2D 클래스로 제공한다.<br>\n",
        "<예시>\n",
        "```\n",
        "from tensorflow import keras\n",
        "keras.layers.Conv2D(10, kernel_size=(3, 3), activation='relu')\n",
        "```\n",
        "Conv2D 클래스의 첫 번째 매개변수는 필터의 개수<br>\n",
        "kernel_size 매개변수는 필터에 사용할 커널의 크기<br>\n",
        "-> 필터 개수와 커널 크기는 반드시 지정해야 하는 매개변수이다.\n",
        "\n",
        "위의 예처럼 합성곱 층을 사용하려면 이전에 Dense 층을 사용했던 자리에 대신 Conv2D 층을 넣으면 된다.\n",
        "\n",
        "합성곱 신경망 : 1개 이상의 합성곱 층을 쓴 인공 신경망(꼭 합성곱 층으로만 이루어지지 않아도 됨)\n",
        "\n"
      ],
      "metadata": {
        "id": "q2Qr-T7daAvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 패딩 & 스트라이드\n",
        "\n",
        "#### <패딩>\n",
        "2차원 입력에 합성곱 연산을 하면 출력(feature map)은 입력의 크기보다 작아진다.<br>\n",
        "그러나 입력과 동일한 크기의 출력(feature map)을 만들고 싶다면, 마치 더 큰 입력에 합성곱하는 척하면 된다.\n",
        "\n",
        "예를 들어 실제 입력 크기는 (4, 4)이지만 (6, 6)처럼 다룬다고 가정하면, (6, 6) 크기의 입력을 (3, 3) 크기의 커널로 합성곱했을 때 출력의 크기는 입력과 동일하게 (4, 4)가 된다.<br>\n",
        "<img src = \"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Ff5147c4d-4432-4874-967b-65c68ff5bd38%2Fimage.png\" height = 650 width = 600><br>\n",
        "이렇게 입력 배열의 주위를 가상의 원소로 채우는 것을 **패딩(padding)**이라고 한다. 이 원소들은 실제 입력값이 아니기 때문에 0으로 채운다. 실제 값은 0이기 때문에 계산에 영향을 미치지는 않고, 패딩의 역할은 순전히 커널이 도장을 찍을 횟수를 늘려주는 것밖에는 없다.<br>\n",
        "<img src = \"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F287dc968-455a-4688-9fc5-c5b416a0d459%2Fimage.png\" height = 250 width = 250>\n",
        "\n",
        "합성곱에서 왜 패딩을 즐겨 사용하는가?<br>\n",
        "만약 패딩이 없다면 입력의 각 꼭짓점 값은 필터가 딱 한 번만 지나간다. 반면 다른 원소들은 2번 이상 커널과 연산되는데, 만약 이 입력을 이미지라고 생각하면 모서리에 있는 중요한 정보가 feature map으로 잘 전달되지 않을 가능성이 높다.<br>\n",
        "때문에 적절한 패딩은 이미지의 가장자리에 있는 정보를 잃어버리지 않도록 한다.<br>\n",
        "케라스의 Conv2D 클래스에서는 padding 매개변수로 패딩을 지정할 수 있다. 기본값은 'valid'로 밸리드 패딩을 나타낸다. 세임 패딩을 사용하려면 'same'으로 지정하면 된다.\n",
        "- 세임 패딩(same padding) : 입력과 특성 맵의 크기를 동일하게 만들기 위해 입력 주위에 0으로 패딩하는 것\n",
        "- 밸리드 패딩(valid padding) : 패딩 없이 순수한 입력에서만 합성곱을 하여 특성 맵을 만드는 것(그냥 패딩 안 한 것)\n",
        "\n",
        "일반적인 합성곱 신경망에서는 세임 패딩 많이 사용\n",
        "```\n",
        "keras.layers.Conv2D(10, kernel_size=(3,3), activation='relu', padding='same')\n",
        "```\n",
        "\n",
        "#### <스트라이드>\n",
        "지금까지 본 합성곱 연산은 필터가 한 칸씩 이동했다. 하지만 이는 고정된 값이 아니다. 두 칸씩 건너뛸 수도 있는데, 이런 이동 크기를 **스트라이드(stride)**라고 한다. 기본적으로 스트라이드는 1이며, 이 값이 케라스 Conv2D의 strides 매개변수의 기본값이다.\n",
        "```\n",
        "keras.layers.Conv2D(10, kernel_size=(3,3), activation='relu', padding='same', strides=1)\n",
        "```\n",
        "스트라이드를 오른쪽으로 이동하는 크기와 아래쪽으로 이동하는 크기를 다르게 지정(튜플로 지정)할 수 있지만, 이런 경우는 거의 없다. 또한 1보다 큰 스트라이드를 사용하는 경우도 드물다. 때문에 strides 매개변수는 잘 사용하지 않는다고 한다."
      ],
      "metadata": {
        "id": "DADxWMIlbfMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 풀링\n",
        "합성곱 층에서 만든 feature map의 가로세로 크기를 줄이는 역할을 한다. 하지만 feature map의 개수는 줄이지 않는다.<br>\n",
        "위의 예를 보면 (2, 2, 3) 크기의 특성 맵에 풀링을 적용하면 마지막 차원인 개수는 그대로 유지하고 너비와 높이만 줄어들어 (1, 1, 3) 크기의 특성 맵이 된다.<br>\n",
        "<img src = \"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Fbcd3555d-e905-4bea-be7c-888278fa310d%2Fimage.png\" height = 350 width = 550>\n",
        "\n",
        "풀링도 합성곱처럼 입력 위를 지나가면서 도장을 찍는다. 하지만 풀링에는 가중치가 없다. 풀링 필터 내의 영역에서 가장 큰 값을 고르거나 평균값을 계산하는데, 이를 각각 **최대 풀링(max pooling)**과 **평균 풀링(average pooling)**이라고 한다.<br>\n",
        "풀링은 합성곱 층과 뚜렷이 구분되기 때문에 풀링층이라고 부른다.<br>\n",
        "\n",
        "다음과 같은 (4, 4) 크기의 feature map이 있다고 가정해보자.<br>\n",
        "<img src = \"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Fb94ce938-0968-464d-9118-e2683626a0be%2Fimage.png\" height = 350 width = 550><br>\n",
        "여기에 (2, 2) 최대 풀링을 적용하면 절반으로 크기가 줄어든다.<br>\n",
        "최대 풀링은 가장 큰 값을 고르기 때문에 첫 번째 (2, 2) 영역에서 9를 고르고 그다음 7, 8, 6을 차례로 골라 (2, 2) 크기의 출력을 만든다. 특성 맵이 여러 개라면 동일한 작업을 반복한다. 즉 10개의 특성 맵이 있다면 풀링을 거친 특성 맵도 10개가 되는 것이다.\n",
        "\n",
        "합성곱에서는 커널이 한 칸씩 이동했기 때문에 겹치는 영역이 있었지만, 풀링에서는 겹치지 않고 이동한다. 따라서 풀링의 크기가 (2, 2)이면 가로세로 두 칸씩 이동한다. 즉 스트라이드가 2이다. 때문에 풀링은 가중치가 없고 패딩도 없으며, 풀링 크기와 스트라이드가 같기 때문에 이해하기 쉽다.\n",
        "\n",
        "케라스에서는 MaxPooling2D 클래스로 풀링을 수행할 수 있다.\n",
        "```\n",
        "keras.layers.MaxPooling2D(2)\n",
        "```\n",
        "MaxPooling2D의 첫 번째 매개변수로 풀링의 크기를 지정한다.(대부분 풀링의 크기는 2, 즉 가로세로 크기를 절반으로 줄인다)<br>\n",
        "튜플을 사용하여 가로세로 방향의 풀링 크기를 다르게 할 수 있지만, 이런 경우는 극히 드물다.<br>\n",
        "합성곱 층과 마찬가지로 strides와 padding 매개변수를 제공하지만, strides의 기본값은 자동으로 풀링의 크기가 되고 풀링은 패딩을 하지 않기 때문에 이 매개변수들을 바꾸는 경우는 거의 없다.\n",
        "\n",
        "평균 풀링을 제공하는 클래스는 AveragePooling2D이다. 최댓값 대신 평균을 계산하는 것만 빼면 MaxPooling2D와 동일하며 제공하는 매개변수도 같지만, 많은 경우 평균 풀링보다 최대 풀링을 많이 사용한다.(평균 풀링은 특성 맵에 있는 중요한 정보를 평균하여 희석시킬 수 있기 때문)\n",
        "\n",
        "<꼭 기억할 점>\n",
        "1. 풀링은 가로세로 방향으로만 진행\n",
        "2. 특성 맵의 개수는 변하지 않고 그대로\n",
        "\n"
      ],
      "metadata": {
        "id": "xM2k6NXDmJW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 합성곱 신경망의 전체 구조\n",
        "\n",
        "합성곱 신경망은 7장처럼 일렬로 늘어선 뉴런으로 표현하기 힘들다. 합성곱 신경망의 입력은 일반적으로 너비와 높이가 있는 이미지이기 때문에 입체적이다.<br>\n",
        "<img src = \"https://velog.velcdn.com/images%2Fboing-86%2Fpost%2F834a2bbe-0e18-4c2d-aee3-e5fbc1efad1a%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-02-24%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%207.33.34.png\" height = 380 width = 800><br>\n",
        "1. 합성곱 층(①)에서 사용할 커널의 크기는 (3, 3) 크기이고 세임 패딩이므로 입력 이미지 주변에 1픽셀이 패딩으로 추가되었다. 이때 패딩은 텐서플로에서 자동으로 추가하므로 수동으로 추가할 필요가 없다. 그다음 패딩이 추가된 입력에서 합성곱이 수행된다.\n",
        "\n",
        "  위 그림에서 합성곱의 필터는 3개이다. 각각 (3, 3) 크기 가중치를 가지고 있으며 필터마다 절편이 하나씩 있다(필터마다 ⓑ가 달린 것이 보인다). 이때 필터의 가중치는 각기 서로 다르다.(밀집층의 뉴런과 동일)\n",
        "\n",
        "  따로 언급하지 않는다면 합성곱의 스트라이드는 항상 1이기 때문에 만들어지는 feature map의 크기는 입력 이미지와 동일한 (4, 4)이다. 3개의 필터가 하나씩 함성곱의 출력을 만들고 이 출력이 합쳐져서 (4, 4, 3) 크기의 3차원 feature map이 만들어진다. 밀집층과 마찬가지로 합성곱 층에서도 활성화 함수를 적용하는데, 합성곱 층은 렐루 함수를 많이 사용한다.\n",
        "\n",
        "2. 풀링 층(②)은 합성곱 층에서 만든 feature map의 가로세로 크기를 줄인다. 보통 (2, 2) 풀링을 사용해 절반으로 줄인다. feature map의 개수는 변하지 않으므로 (4, 4, 3)에서 (2, 2, 3)으로 feature map 개수는 유지된 것이 보인다.\n",
        "\n",
        "  풀링을 사용하는 이유는 합성곱에서 스트라이드를 크게 하여 특성 맵을 줄이는 것보다 풀링 층에서 크기를 줄이는 것이 경험적으로 더 나은 성능을 내기 때문이다.\n",
        "\n",
        "  합성곱 신경망은 이렇게 합성곱 층에서 특성 맵을 생성하고 풀링에서 크기를 줄이는 구조가 쌍을 이룬다.\n",
        "  \n",
        "3. (2, 2, 3)으로 줄어든 feature map을 밀집층(③)인 출력층에 전달하려면 이 3차원 feature map을 1차원으로 펼쳐야 한다(이때는 7장에서 배운 Flatten 클래스 사용).<br>\n",
        "따라서 이 배열은 12개의 원소를 가진 1차원 배열이고 출력층의 입력이 된다.\n",
        "\n",
        "  출력층에는 3개의 뉴런을 두었다. 즉 3개의 클래스를 분류하는 다중 분류 모델이다. 출력층에서 계산된 값은 다중 분류 활성화 함수인 소프트맥스 함수를 거쳐 최종 예측 확률이 된다.\n"
      ],
      "metadata": {
        "id": "E9QylFlfEn57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 컬러 이미지를 사용한 합성곱\n",
        "\n",
        "지금까지 우리는 입력을 2차원 배열이라고 가정했다. 그러나 이것은 흑백 이미지일 경우에만 해당된다.<br>\n",
        "컬러 이미지는 RGB 채널로 구성되어 있기 때문에 컴퓨터는 이를 3차원 배열로 표시한다.\n",
        "\n",
        "즉 하나의 컬러 이미지는 너비와 높이 차원 외에 깊이 차원이 추가로 존재한다. 예를 들면 앞의 예제에서 입력이 (4, 4)가 아니라 (4, 4, 3)이 되는 것이다.\n",
        "\n",
        "이런 경우에 깊이가 있는 입력에서 합성곱을 수행하기 위해서는 필터도 깊이가 필요하다. 즉 필터의 커널 크기가 (3, 3)이 아니라 (3, 3, 3)이 된다. 결국 커널 배열의 깊이는 항상 입력의 깊이와 같다.<br>\n",
        "<img src = \"https://velog.velcdn.com/images/sorij95/post/6223fc8d-57df-4b71-96eb-ea41fbb3d6a6/image.png\" height = 300 width = 300><br>\n",
        "이 합성곱의 계산은 (3, 3, 3) 영역에 해당하는 27개의 원소에 27개의 가중치를 곱하고 절편을 더하는 식이 된다.\n",
        "\n",
        "여기서 중요한 것! 이력이나 필터의 차원이 몇 개인지 상관없이 항상 출력은 하나의 값이라는 점이다.<br>\n",
        "<img src = \"https://velog.velcdn.com/images/lancia01/post/7d3093ba-b360-4c54-b173-6a8a8c00a9fd/image.png\" height = 250 width = 350>\n",
        "\n",
        "사실 케라스의 합성곱 층은 항상 이렇게 3차원 입력을 기대한다. 만약 흑백 이미지일 경우에는 깊이 차원이 1인 3차원 배열로 변환하여 전달한다.<br>\n",
        "예를 들어 (28, 28) 크기의 2차원 배열을 (28, 28, 1) 크기의 3차원 배열로 변환하는 것이다. 즉 원소 개수는 동일하면서 차원만 맞춘 것이다.\n",
        "\n",
        "이와 비슷한 경우가 또 있는데, 바로 합성곱 층-풀링 층 다음에 다시 또 합성곱 층이 올 때이다. 예를 들어 첫 번째 합성곱 층의 필터 개수가 5개라고 가정하여 첫 번째 풀링 층을 통과한 특성 맵의 크기가 (4, 4, 5)라고 해보자.<br>\n",
        "두 번째 합성곱 층에서 필터의 너비와 높이가 각각 3이라면 이 필터의 커널 크기는 (3, 3, 5)가 된다. 입력의 깊이와 필터의 깊이는 같아야 하기 때문이다. 다음 그림처럼 3 x 3 x 5 = 45개의 가중치를 곱하고 절편을 더한 이 합성곱의 결과는 1개의 출력을 만든다.<br>\n",
        "두 번째 합성곱 층의 필터 개수가 10개라면 만들어진 특성 맵의 크기는 (2, 2, 10)이 될 것이다.\n",
        "\n",
        "이렇게 합성곱 신경망은 너비와 높이는 점점 줄어들고 깊이는 점점 깊어지는 것이 특징이다. 그리고 마지막에 출력층 전에 특성 맵을 모두 펼쳐서 밀집층의 입력으로 사용한다."
      ],
      "metadata": {
        "id": "sMVpIE7IIi5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "합성곱 신경망에서 필터는 이미지에 있는 어떤 특징을 찾는다고 생각할 수 있다. 처음에는 간단한 특징(직선, 곡선 등)을 찾고 층이 깊어질수록 다양하고 구체적인 특징을 감지할 수 있도록 필터의 개수를 늘린다(층을 깊게 만들기 위해). 또 어떤 특징이 이미지의 어느 위치에 놓이더라도 쉽게 감지할 수 있도록 너비와 높이 차원을 점점 줄여가는 것이다."
      ],
      "metadata": {
        "id": "P3N7TxAtW_aX"
      }
    }
  ]
}